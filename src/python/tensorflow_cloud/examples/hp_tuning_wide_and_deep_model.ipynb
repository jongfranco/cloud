{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaYLYwXyC99u"
      },
      "source": [
        "# Tuning a wide and Deep model using Google Cloud\n",
        "\n",
        "In this example we will use CloudTuner and Google Cloud to Tune a [Wide and Deep Model](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) based on the tunable model introduced in [structured data learning with Wide, Deep, and Cross networks](https://keras.io/examples/structured_data/wide_deep_cross_networks/). In this example we will use the data set from [CAIIS Dogfood Day](https://www.kaggle.com/c/caiis-dogfood-day-2020/overview)\n",
        "\n",
        "\u003ctable align=\"left\"\u003e\n",
        "    \u003ctd\u003e\n",
        "        \u003ca href=\"https://colab.research.google.com/github/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/examples/hp_tuning_wide_and_deep_model.ipynb\"\u003e\n",
        "            \u003cimg width=\"50\" src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"\u003eRun in Colab\n",
        "        \u003c/a\u003e\n",
        "    \u003c/td\u003e\n",
        "    \u003ctd\u003e\n",
        "        \u003ca href=\"https://github.com/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/examples/hp_tuning_wide_and_deep_model.ipynb\"\u003e\n",
        "            \u003cimg src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"\u003eView on GitHub\n",
        "        \u003c/a\u003e\n",
        "     \u003c/td\u003e\n",
        "    \u003ctd\u003e\n",
        "        \u003ca href=\"https://www.kaggle.com/nitric/hp-tuning-wide-and-deep-model\"\u003e\n",
        "            \u003cimg width=\"90\" src=\"https://www.kaggle.com/static/images/site-logo.png\" alt=\"Kaggle logo\"\u003eRun in Kaggle\n",
        "        \u003c/a\u003e\n",
        "     \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzyeZrh9C99v",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import uuid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_cloud as tfc\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDbbaIEIC99v",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if not tfc.remote():\n",
        "    print(tfc.__version__)\n",
        "    if tfc.__version__ \u003c '0.1.12':\n",
        "        raise RuntimeError(\"This example requires tensorflow_cloud version 0.1.12 or newer!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbGPwAkhC99v"
      },
      "source": [
        "Setting project parameters. For more details on Google Cloud Specific parameters please refer to [Google Cloud Project Setup Instructions](https://www.kaggle.com/nitric/google-cloud-project-setup-instructions/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P-V4o-ZC99w",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Set Google Cloud Specific parameters\n",
        "\n",
        "# TODO: Please set GCP_PROJECT_ID to your own Google Cloud project ID.\n",
        "GCP_PROJECT_ID = 'YOUR_PROJECT_ID'  #@param {type:\"string\"}\n",
        "\n",
        "# TODO: Change the Service Account Name to your own Service Account\n",
        "SERVICE_ACCOUNT_NAME = 'YOUR_SERVICE_ACCOUNT_NAME' #@param {type:\"string\"}\n",
        "SERVICE_ACCOUNT = f'{SERVICE_ACCOUNT_NAME}@{GCP_PROJECT_ID}.iam.gserviceaccount.com'\n",
        "\n",
        "# TODO: set GCS_BUCKET to your own Google Cloud Storage (GCS) bucket.\n",
        "GCS_BUCKET = 'YOUR_GCS_BUCKET_NAME' #@param {type:\"string\"}\n",
        "\n",
        "# DO NOT CHANGE: Currently only the 'us-central1' region is supported.\n",
        "REGION = 'us-central1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlnssL5zC99w",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Set Tuning Specific parameters\n",
        "\n",
        "# OPTIONAL: You can change the project name to any string.\n",
        "JOB_NAME = 'wide_and_deep' #@param {type:\"string\"}\n",
        "\n",
        "# OPTIONAL:  Set Number of concurrent tuning jobs that you would like to run.\n",
        "NUM_JOBS = 5 #@param {type:\"string\"}\n",
        "\n",
        "# TODO: Set the study ID for this run. Study_ID can be any unique string.\n",
        "# Reusing the same Study_ID will cause the Tuner to continue tuning the\n",
        "# Same Study parameters. This can be used to continue on a terminated job,\n",
        "# or load stats from a previous study.\n",
        "# Note Study ID MUST NOT be auto generated for example using random or time,\n",
        "# as at run time each job will rerun this code and will create a separate study.\n",
        "STUDY_NUMBER = '00001' #@param {type:\"string\"}\n",
        "STUDY_ID = f'{GCP_PROJECT_ID}_{JOB_NAME}_{STUDY_NUMBER}'\n",
        "\n",
        "# Setting location were training logs and checkpoints will be stored\n",
        "GCS_BASE_PATH = f'gs://{GCS_BUCKET}/{JOB_NAME}/{STUDY_ID}'\n",
        "TENSORBOARD_LOGS_DIR = os.path.join(GCS_BASE_PATH,\"logs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcnAEBZ3C99w"
      },
      "source": [
        "## Authenticating the notebook to use your Google Cloud Project\n",
        "\n",
        "For Kaggle Notebooks click on \"Add-ons\"-\u003e\"Google Cloud SDK\" before running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jww_Z_ATC99x",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Using tfc.remote() to ensure this code only runs in notebook\n",
        "if not tfc.remote():\n",
        "\n",
        "    # Authentication for Kaggle Notebooks\n",
        "    if \"kaggle_secrets\" in sys.modules:\n",
        "        from kaggle_secrets import UserSecretsClient\n",
        "        UserSecretsClient().set_gcloud_credentials(project=GCP_PROJECT_ID)\n",
        "\n",
        "    # Authentication for Colab Notebooks\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth\n",
        "        auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGfKykt1C99x"
      },
      "source": [
        "## Load the data\n",
        "Read raw data and split to train and test data sets. For this step you will need to copy the dataset to your GCS bucket so it can be accessed during training. For this example we are using the dataset from https://www.kaggle.com/c/caiis-dogfood-day-2020.\n",
        "\n",
        "To do this you can run the following commands to download and copy the dataset to your GCS bucket, or manually download the dataset vi [Kaggle UI](https://www.kaggle.com/c/caiis-dogfood-day-2020/data) and upload the `train.csv` file to your [GCS bucket vi GCS UI](https://console.cloud.google.com/storage/browser).\n",
        "\n",
        "```python\n",
        "# Download the dataset\n",
        "!kaggle competitions download -c caiis-dogfood-day-2020\n",
        "\n",
        "# Copy the training file to your bucket\n",
        "!gsutil cp ./caiis-dogfood-day-2020/train.csv $GCS_BASE_PATH/caiis-dogfood-day-2020/train.csv\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_M3pYKAC99x",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_URL = f'{GCS_BASE_PATH}/caiis-dogfood-day-2020/train.csv'\n",
        "data = pd.read_csv(train_URL)\n",
        "train, test = train_test_split(data, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vymkdn1cC99y",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
        "def df_to_dataset(df, shuffle=True, batch_size=32):\n",
        "  df = df.copy()\n",
        "  labels = df.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(df))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw0ulZuBC99y",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sm_batch_size = 1000  # A small batch size is used for demonstration purposes\n",
        "train_ds = df_to_dataset(train, batch_size=sm_batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=sm_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoWO7nGZC99y"
      },
      "source": [
        "## Preprocess the data\n",
        "\n",
        "Setting up preprocessing layers for categorical and numerical input data. For more details on preprocessing layers please refer to [working with preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hACoYJOC99z",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "def create_model_inputs():\n",
        "    inputs ={}\n",
        "    for name, column in data.items():\n",
        "        if name in ('id','target'):\n",
        "            continue\n",
        "        dtype = column.dtype\n",
        "        if dtype == object:\n",
        "            dtype = tf.string\n",
        "        else:\n",
        "            dtype = tf.float32\n",
        "\n",
        "        inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)\n",
        "\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaDiPMy3C99z",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Preprocessing the numeric inputs, and runing them through a normalization layer.\n",
        "def preprocess_numeric_inputs(inputs):\n",
        "\n",
        "    numeric_inputs = {name:input for name,input in inputs.items()\n",
        "                      if input.dtype==tf.float32}\n",
        "\n",
        "    x = layers.Concatenate()(list(numeric_inputs.values()))\n",
        "    norm = preprocessing.Normalization()\n",
        "    norm.adapt(np.array(data[numeric_inputs.keys()]))\n",
        "    numeric_inputs = norm(x)\n",
        "    return numeric_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWSrlPu-N_01"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the categorical inputs.\n",
        "def preprocess_categorical_inputs(inputs):\n",
        "    categorical_inputs = []\n",
        "    for name, input in inputs.items():\n",
        "        if input.dtype == tf.float32:\n",
        "            continue\n",
        "\n",
        "        lookup = preprocessing.StringLookup(vocabulary=np.unique(data[name]))\n",
        "        one_hot = preprocessing.CategoryEncoding(max_tokens=lookup.vocab_size())\n",
        "\n",
        "        x = lookup(input)\n",
        "        x = one_hot(x)\n",
        "        categorical_inputs.append(x)\n",
        "\n",
        "    return layers.concatenate(categorical_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGuCZrqXC99z"
      },
      "source": [
        "## Define the model architecture and hyperparameters\n",
        "Now we will define the model structure and the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O81jYLSgC990",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import kerastuner\n",
        "\n",
        "# Configure the search space\n",
        "HPS = kerastuner.engine.hyperparameters.HyperParameters()\n",
        "HPS.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
        "\n",
        "HPS.Int('num_layers', min_value=2, max_value=5)\n",
        "for i in range(5):\n",
        "    HPS.Float('dropout_rate_' + str(i), min_value=0.0, max_value=0.3, step=0.1)\n",
        "    HPS.Choice('num_units_' + str(i), [32, 64, 128, 256])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZbO-QN4C990",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "def create_wide_and_deep_model(hp):\n",
        "\n",
        "    inputs = create_model_inputs()\n",
        "    wide = preprocess_categorical_inputs(inputs)\n",
        "    wide = layers.BatchNormalization()(wide)\n",
        "\n",
        "    deep = preprocess_numeric_inputs(inputs)\n",
        "    for i in range(hp.get('num_layers')):\n",
        "        deep = layers.Dense(hp.get('num_units_' + str(i)))(deep)\n",
        "        deep = layers.BatchNormalization()(deep)\n",
        "        deep = layers.ReLU()(deep)\n",
        "        deep = layers.Dropout(hp.get('dropout_rate_' + str(i)))(deep)\n",
        "\n",
        "    both = layers.concatenate([wide, deep])\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(both)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    metrics = [\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "        'accuracy',\n",
        "        'mse'\n",
        "    ]\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(lr=hp.get('learning_rate')),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=metrics)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PULhnujAC991"
      },
      "source": [
        "## Configure a CloudTuner\n",
        "In this section we configure the CloudTuner for both remote and local execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpAipYOcC991",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tensorflow_cloud import CloudTuner\n",
        "\n",
        "tuner = CloudTuner(\n",
        "    create_wide_and_deep_model,\n",
        "    project_id=GCP_PROJECT_ID,\n",
        "    project_name=JOB_NAME,\n",
        "    region=REGION,\n",
        "    objective='accuracy',\n",
        "    hyperparameters=HPS,\n",
        "    max_trials=100,\n",
        "    directory=GCS_BASE_PATH,\n",
        "    study_id=STUDY_ID,\n",
        "    overwrite=False,\n",
        "    distribution_strategy=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsEXQTsIC991",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Configure Tensorboard logs\n",
        "callbacks=[\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=TENSORBOARD_LOGS_DIR)]\n",
        "\n",
        "# Setting to run tuning remotely, you can run tuner locally to validate it works first.\n",
        "if tfc.remote():\n",
        "    tuner.search(train_ds, epochs=20, validation_data=test_ds, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdYhCN52C992"
      },
      "source": [
        "## Start the remote training\n",
        "\n",
        "This step will prepare your code from this notebook for remote execution and start NUM_JOBS parallel runs remotely to train the model. Once the jobs are submitted you can go to the next step to monitor the jobs progress via Tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6YlV2KOC992",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if not tfc.remote():\n",
        "    print('Training on Google Cloud AI Platform through TensorFlow Cloud.')\n",
        "\n",
        "    tfc.run_cloudtuner(\n",
        "        distribution_strategy='auto',\n",
        "        docker_config=tfc.DockerConfig(\n",
        "            image_build_bucket=GCS_BUCKET\n",
        "            ),\n",
        "        chief_config=tfc.MachineConfig(\n",
        "            cpu_cores=16,\n",
        "            memory=60,\n",
        "        ),\n",
        "        job_labels={'job': JOB_NAME},\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "        num_jobs=NUM_JOBS\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0k_9LRMC992"
      },
      "source": [
        "# Training Results\n",
        "While the training is in progress you can use Tensorboard to view the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kIsuByATC992",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if not tfc.remote():\n",
        "\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir TENSORBOARD_LOGS_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYwc5PpiC992"
      },
      "source": [
        "You can access the training assets as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "K7ELIwXHC993",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if not tfc.remote():\n",
        "    tuner.results_summary(1)\n",
        "    best_model = tuner.get_best_models(1)[0]\n",
        "    best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
        "\n",
        "    # References to best trial assets\n",
        "    best_trial_id = tuner.oracle.get_best_trials(1)[0].trial_id\n",
        "    best_trial_dir = tuner.get_trial_dir(best_trial_id)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "hp-tuning-wide-and-deep-model.ipynb",
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/tensorflow_cloud/src/python/tensorflow_cloud/examples/hp_tuning_wide_and_deep_model.ipynb?workspaceId=chavoshi:Sample_notebooks::citc",
          "timestamp": 1612378497201
        },
        {
          "file_id": "/piper/depot/google3/third_party/tensorflow_cloud/src/python/tensorflow_cloud/examples/hp_tuning_wide_and_deep_model.ipynb?workspaceId=chavoshi:Sample_notebooks::citc",
          "timestamp": 1612378139403
        },
        {
          "file_id": "/piper/depot/google3/third_party/tensorflow_cloud/src/python/tensorflow_cloud/examples/hp_tuning_wide_and_deep_model.ipynb?workspaceId=chavoshi:Sample_notebooks::citc",
          "timestamp": 1612316169032
        },
        {
          "file_id": "/piper/depot/google3/third_party/tensorflow_cloud/src/python/tensorflow_cloud/examples/hp_tuning_wide_and_deep_model.ipynb?workspaceId=chavoshi:Sample_notebooks::citc",
          "timestamp": 1612307918898
        },
        {
          "file_id": "/piper/depot/google3/third_party/tensorflow_cloud/src/python/tensorflow_cloud/examples/hp_tuning_wide_and_deep_model.ipynb?workspaceId=chavoshi:Sample_notebooks::citc",
          "timestamp": 1612304250599
        },
        {
          "file_id": "/piper/depot/google3/third_party/tensorflow_cloud/src/python/tensorflow_cloud/examples/hp_tuning_wide_and_deep_model.ipynb?workspaceId=chavoshi:Sample_notebooks::citc",
          "timestamp": 1612304192631
        },
        {
          "file_id": "/piper/depot/google3/third_party/tensorflow_cloud/src/python/tensorflow_cloud/examples/hp_tuning_wide_and_deep_model.ipynb?workspaceId=chavoshi:Sample_notebooks::citc",
          "timestamp": 1612303991064
        },
        {
          "file_id": "12ZziX9PYMwyXHRc3XO6Y3M6Q9WoPEHgW",
          "timestamp": 1612157034678
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
